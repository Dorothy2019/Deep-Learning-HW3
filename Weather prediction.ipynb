{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weather prediction using deep neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First I need to read the CSV file, and the float number which are represented in the following format: 10,2 convert to 10.2 and also change its type from str to float."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Day  Tmax  Tmin  Tmean\n",
      "0    1.57784e+09     3   0.2    1.6\n",
      "1    1.57792e+09   3.3  -0.5    1.4\n",
      "2    1.57801e+09   2.4  -0.7   0.85\n",
      "3     1.5781e+09     1   1.1   1.05\n",
      "4    1.57818e+09   3.5  -0.7    1.4\n",
      "..           ...   ...   ...    ...\n",
      "293  1.60315e+09  14.8   9.6   12.2\n",
      "294  1.60324e+09  16.6   8.9  12.75\n",
      "295  1.60332e+09  15.7  10.3     13\n",
      "296  1.60341e+09  16.4  10.8   13.6\n",
      "297   1.6035e+09  17.9  12.1     15\n",
      "\n",
      "[298 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import csv\n",
    "\n",
    "weather_data = pd.read_csv(\"weather_data.csv\",sep=';')\n",
    "\n",
    "features = ['Day','Tmax','Tmin','Tmean'] # the actal day when the measure was, Temperature max, T.min, Average\n",
    "df = pd.DataFrame(weather_data, columns=features)\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    row[0] = pd.to_datetime(row[0])\n",
    "    row[0] = row[0].timestamp()\n",
    "    row[1] = float((row[1]).replace(',','.')) #converting the strings to float numbers\n",
    "    row[2] = float((row[2]).replace(',','.'))\n",
    "    row[3] = float((row[3]).replace(',','.'))\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now I have the cleared data I will create an LSTM network\n",
    "I will use 80% of the data to train, 10% for test and 10% for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training instances   (238, 3), Training features   (238, 3)\n",
      "Validation instances (30, 3), Validation features (30, 3)\n",
      "Testing instances    (30, 3), Testing features    (30, 3)\n",
      "Epoch 1/20\n",
      "238/238 [==============================] - 2s 9ms/step - loss: 216.3497 - acc: 0.0000e+00 - val_loss: 157.2217 - val_acc: 0.0000e+00\n",
      "Epoch 2/20\n",
      "238/238 [==============================] - 1s 3ms/step - loss: 121.6151 - acc: 0.0000e+00 - val_loss: 101.5381 - val_acc: 0.0000e+00\n",
      "Epoch 3/20\n",
      "238/238 [==============================] - 1s 3ms/step - loss: 93.3105 - acc: 0.0000e+00 - val_loss: 84.4230 - val_acc: 0.0000e+00\n",
      "Epoch 4/20\n",
      "238/238 [==============================] - 1s 3ms/step - loss: 84.0453 - acc: 0.0000e+00 - val_loss: 78.7966 - val_acc: 0.0000e+00\n",
      "Epoch 5/20\n",
      "238/238 [==============================] - 1s 3ms/step - loss: 81.8632 - acc: 0.0000e+00 - val_loss: 75.9374 - val_acc: 0.0000e+00\n",
      "Epoch 6/20\n",
      "238/238 [==============================] - 1s 3ms/step - loss: 51.7735 - acc: 0.0000e+00 - val_loss: 37.1896 - val_acc: 0.0000e+00\n",
      "Epoch 7/20\n",
      "238/238 [==============================] - 1s 3ms/step - loss: 32.5596 - acc: 0.0000e+00 - val_loss: 27.0913 - val_acc: 0.0000e+00\n",
      "Epoch 8/20\n",
      "238/238 [==============================] - 1s 3ms/step - loss: 19.0427 - acc: 0.0000e+00 - val_loss: 13.1548 - val_acc: 0.0000e+00\n",
      "Epoch 9/20\n",
      "238/238 [==============================] - 1s 3ms/step - loss: 12.8427 - acc: 0.0000e+00 - val_loss: 8.1588 - val_acc: 0.0000e+00\n",
      "Epoch 10/20\n",
      "238/238 [==============================] - 1s 3ms/step - loss: 9.7804 - acc: 0.0042 - val_loss: 6.9742 - val_acc: 0.0000e+00\n",
      "Epoch 11/20\n",
      "238/238 [==============================] - 1s 3ms/step - loss: 7.4105 - acc: 0.0000e+00 - val_loss: 4.1414 - val_acc: 0.0000e+00\n",
      "Epoch 12/20\n",
      "238/238 [==============================] - 1s 3ms/step - loss: 5.8692 - acc: 0.0000e+00 - val_loss: 3.1125 - val_acc: 0.0000e+00\n",
      "Epoch 13/20\n",
      "238/238 [==============================] - 1s 3ms/step - loss: 5.7290 - acc: 0.0000e+00 - val_loss: 3.1865 - val_acc: 0.0000e+00\n",
      "Epoch 14/20\n",
      "238/238 [==============================] - 1s 3ms/step - loss: 5.3807 - acc: 0.0000e+00 - val_loss: 2.3819 - val_acc: 0.0000e+00\n",
      "Epoch 15/20\n",
      "238/238 [==============================] - 1s 3ms/step - loss: 5.8731 - acc: 0.0084 - val_loss: 1.7739 - val_acc: 0.0000e+00\n",
      "Epoch 16/20\n",
      "238/238 [==============================] - 1s 3ms/step - loss: 5.5465 - acc: 0.0042 - val_loss: 2.0401 - val_acc: 0.0000e+00\n",
      "Epoch 17/20\n",
      "238/238 [==============================] - 1s 3ms/step - loss: 4.3227 - acc: 0.0000e+00 - val_loss: 1.9044 - val_acc: 0.0000e+00\n",
      "Epoch 18/20\n",
      "238/238 [==============================] - 1s 3ms/step - loss: 3.9866 - acc: 0.0000e+00 - val_loss: 1.3532 - val_acc: 0.0000e+00\n",
      "Epoch 19/20\n",
      "238/238 [==============================] - 1s 3ms/step - loss: 4.8654 - acc: 0.0000e+00 - val_loss: 1.2361 - val_acc: 0.0000e+00\n",
      "Epoch 20/20\n",
      "238/238 [==============================] - 1s 3ms/step - loss: 3.4730 - acc: 0.0000e+00 - val_loss: 0.9981 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x238bc2d93a0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Bidirectional, LSTM, Dropout\n",
    "\n",
    "X = df[[col for col in df.columns if col not in ['Tmean']]]\n",
    "y = df['Tmean']\n",
    "\n",
    "\n",
    "X_train, X_other, y_train, y_other = train_test_split(X,y, train_size=0.8, random_state=23)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_other, y_other, test_size=0.5, random_state=23)\n",
    "X_train.shape, X_test.shape, X_val.shape\n",
    "print(f\"Training instances   {X_train.shape}, Training features   {X_train.shape}\")\n",
    "print(f\"Validation instances {X_val.shape}, Validation features { X_val.shape}\")\n",
    "print(f\"Testing instances    {X_test.shape}, Testing features    {X_test.shape}\")\n",
    "\n",
    "\n",
    "#Convert data to numpy array\n",
    "X_train = X_train.to_numpy()\n",
    "y_train = y_train.to_numpy()\n",
    "X_test = X_test.to_numpy()\n",
    "y_test = y_test.to_numpy()\n",
    "X_val= X_val.to_numpy()\n",
    "y_val = y_val.to_numpy()\n",
    "\n",
    "#Reshape input data to have 3 dimensions\n",
    "X_train = X_train.reshape(-1,3,1)\n",
    "X_test = X_test.reshape(-1,3,1)\n",
    "X_val= X_val.reshape(-1,3,1)\n",
    "\n",
    "\n",
    "#Converting data to tensor\n",
    "X_train_tensor = tf.convert_to_tensor(X_train, dtype=float)\n",
    "y_train_tensor = tf.convert_to_tensor(y_train,dtype=float)\n",
    "X_test_tensor = tf.convert_to_tensor(X_test,dtype=float)\n",
    "y_test_tensor = tf.convert_to_tensor(y_test,dtype=float)\n",
    "X_val_tensor = tf.convert_to_tensor(X_val,dtype=float)\n",
    "y_val_tensor = tf.convert_to_tensor(y_val, dtype=float)\n",
    "\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(units=30, return_sequences=True, input_shape = (X_train_tensor.shape[0], X_train_tensor.shape[1],1))))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(units= 30, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(units= 30, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(units= 30))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(units = 30, activation='relu'))\n",
    "\n",
    "model.compile(optimizer='adam',loss='mean_squared_error',metrics=['acc'])\n",
    "\n",
    "model.fit(X_train_tensor,y_train_tensor, epochs=EPOCHS,batch_size=BATCH_SIZE, validation_data=(X_val_tensor,y_val_tensor))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting temperature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000238C9F53CA0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Predicted temperature for 28.october : 12.25357437133789\n",
      "Predicted temperature for 3.november : 24.650575637817383\n",
      "Predicted temperature for 24.november : 21.069730758666992\n"
     ]
    }
   ],
   "source": [
    "predicted_temperature = model.predict(X_test_tensor)\n",
    "print(f\"Predicted temperature for 28.october : {np.mean(predicted_temperature[0],axis=0) }\")\n",
    "print(f\"Predicted temperature for 3.november : {np.mean(predicted_temperature[6],axis=0) }\")\n",
    "print(f\"Predicted temperature for 24.november : {np.mean(predicted_temperature[29],axis=0)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
